{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f701a4cf",
   "metadata": {},
   "source": [
    "# Assignment 2 - Scott Wilkinson (V00887986) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b5487",
   "metadata": {},
   "source": [
    "## Bonus: Creating a pipeline for Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c90f5",
   "metadata": {},
   "source": [
    "A notebook which uses the GridSearch function to obtain both the optimal hyperparameters AND optimal data preparation choices (e.g. n_components for PCA) for a logistic regression model to classify simulated galaxies from Illustris TNG as pre-mergers or post-mergers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd78af",
   "metadata": {},
   "source": [
    "First, we must import the necessary Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e335a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:31: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "# importing packages used in notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymysql, os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# deprecation warnings make the notebook hard to read but do not affect results\n",
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "# going full nuclear\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba76c9e6",
   "metadata": {},
   "source": [
    "## Importing Morphology Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5878c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 219933)\n"
     ]
    }
   ],
   "source": [
    "#Query SQL for morphology data\n",
    "# connects to database\n",
    "db = pymysql.connect(host = 'lauca.phys.uvic.ca', db = 'sdss', user = 'swilkinson', passwd = '123Sdss!@#')\n",
    "# select morphology params that have unflagged morphology fits\n",
    "x = 'SELECT  dbID, asymmetry, shape_asymmetry, gini_m20_merger, concentration, outer_asymmetry, deviation, multimode, intensity \\\n",
    "    FROM simCFIS_morph\\\n",
    "    WHERE flag_morph = 0\\\n",
    "    AND asymmetry > -1'\n",
    "c = db.cursor()\n",
    "c.execute(x)\n",
    "db_data = c.fetchall()\n",
    "c.close()\n",
    "db.close()\n",
    "\n",
    "# save names as a string\n",
    "names_morph = np.array(db_data, dtype = str).T[0]\n",
    "\n",
    "# save rest of data as floats\n",
    "morph = np.array(db_data, dtype = float).T[1:]\n",
    "\n",
    "print(morph.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914afcf6",
   "metadata": {},
   "source": [
    "# Merger Sample Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6a2ec",
   "metadata": {},
   "source": [
    "## Selecting a Sample of Pre-Merger Galaxies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e60b7",
   "metadata": {},
   "source": [
    "Here is the reasoning behind my selection cuts:\n",
    "\n",
    "1.  snap >= 50 requires z<1 in the simulation (ie. enough time for galaxies to form, evolve and begin to merge)\n",
    "2. rsep > 0 requires there to be two merging galaxies that are not on top of each other yet\n",
    "3. mass ratio > 0.1, the merger will be significant when it occurs\n",
    "4. Flaguntilmerger = 1 ensures the merger is true and not a projection effect that confuses the `subfind` code (see Hani et al. 2020, Rodriguez-Gomez et al. 2015)\n",
    "5. Tuntilmerger < 0.5 ensures the merger will happen soon (< 500 Myr) and the galaxies have begun to gravitationally interact\n",
    "6. Tpostmerger > 0.5 ensures no recent has occurred that would cause the disturbed morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df5a3774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6985 pre-merger galaxies.\n",
      "There are 2783 pre-merger galaxies with flag-free morphology data.\n"
     ]
    }
   ],
   "source": [
    "#Query MySQL with reasoning described above\n",
    "db = pymysql.connect(host = 'lauca.phys.uvic.ca', db = 'IllustrisTNG100_1', user = 'swilkinson', passwd = '123Sdss!@#')\n",
    "x ='SELECT e.DB_ID, e.Mstar, e.Tuntilmerger, e.MassRatiountilmerger\\\n",
    "    FROM Environment e \\\n",
    "    WHERE snapNum>=50\\\n",
    "    AND rsep>0 \\\n",
    "    AND MassRatiountilmerger>=0.1\\\n",
    "    AND Flaguntilmerger = 1\\\n",
    "    AND Tuntilmerger < 0.5\\\n",
    "    AND Tpostmerger > 0.5'\n",
    "c = db.cursor()\n",
    "c.execute(x)\n",
    "db_data = c.fetchall()\n",
    "c.close()\n",
    "db.close()\n",
    "\n",
    "names_db_pre = np.array(db_data, dtype = str).T[0]\n",
    "Mstar_pre = np.array(db_data, dtype = float).T[1]\n",
    "Tum_pre = np.array(db_data, dtype = float).T[2]\n",
    "mu_pre = np.array(db_data, dtype = float).T[3]\n",
    "\n",
    "# give time before merger a negative value\n",
    "Tum_pre *= -1\n",
    "\n",
    "# differentiate pre-merger galaxies that merge within the next snapshot from post-merger galaxies\n",
    "#   that have merged within the last snap shot by adding / subtracting 0.1Gyr (approx the middle of a snapshot)\n",
    "Tum_pre[Tum_pre == 0] = -0.05\n",
    "\n",
    "# add 0 to match formatting with morphology catalogue\n",
    "names_db_pre = np.array(['0'+n for n in names_db_pre])\n",
    "\n",
    "print('There are {} pre-merger galaxies.'.format(len(names_db_pre)))\n",
    "\n",
    "# match the two catalogues\n",
    "match, idx_morph_pre, idx_pre = np.intersect1d(names_morph, names_db_pre, return_indices = True)\n",
    "\n",
    "print('There are {} pre-merger galaxies with flag-free morphology data.'.format(len(names_db_pre[idx_pre])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecf2a36",
   "metadata": {},
   "source": [
    "## Selecting a Sample of Post-Merger Galaxies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30caca",
   "metadata": {},
   "source": [
    "Here is the reasoning behind my selection cuts:\n",
    "\n",
    "1. snap >= 50 requires z<1 in the simulation (ie. enough time for galaxies to form, evolve and begin to merge)\n",
    "2. mass ratio > 0.1, the merger that occurred was significant\n",
    "3. Flagpostmerger = 1 ensures the merger is true and not a projection effect that confuses the `subfind` code\n",
    "4. Tpostmerger < 0.5 requires a merger to have occurred in the last 500 Myr\n",
    "5. rsep > 25 and Tuntilmerger < 0.5 requires there to be no nearby galaxies about to cause another merger in the next 500 Myr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f67c019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4853 post-merger galaxies.\n",
      "There are 3192 post-merger galaxies with flag-free morphology data.\n"
     ]
    }
   ],
   "source": [
    "#Query MySQL with the reasoning listed above\n",
    "db = pymysql.connect(host = 'lauca.phys.uvic.ca', db = 'IllustrisTNG100_1', user = 'swilkinson', passwd = '123Sdss!@#')\n",
    "x ='SELECT e.DB_ID, e.Mstar, e.Tpostmerger, e.MassRatio\\\n",
    "    FROM Environment e \\\n",
    "    WHERE snapNum>=50\\\n",
    "    AND MassRatio>=0.1\\\n",
    "    AND Flagpostmerger = 1\\\n",
    "    AND Tpostmerger < 0.5\\\n",
    "    AND rsep>25\\\n",
    "    AND Tuntilmerger > 0.5'\n",
    "c = db.cursor()\n",
    "c.execute(x)\n",
    "db_data = c.fetchall()\n",
    "c.close()\n",
    "db.close()\n",
    "\n",
    "names_db_post = np.array(db_data, dtype = str).T[0]\n",
    "Mstar_post = np.array(db_data, dtype = float).T[1]\n",
    "Tpm_post = np.array(db_data, dtype = float).T[2]\n",
    "mu_post = np.array(db_data, dtype = float).T[3]\n",
    "\n",
    "# add 0 to match formatting with morphology catalogue\n",
    "names_db_post = np.array(['0'+n for n in names_db_post])\n",
    "\n",
    "# differentiate pre-merger galaxies that merge within the next snapshot from post-merger galaxies\n",
    "#   that have merged within the last snap shot by adding / subtracting 0.1Gyr (approx the middle of a snapshot)\n",
    "Tpm_post[Tpm_post == 0] = 0.05\n",
    "\n",
    "print('There are {} post-merger galaxies.'.format(len(names_db_post)))\n",
    "\n",
    "# match with morphology catalogue\n",
    "match, idx_morph_post, idx_post = np.intersect1d(names_morph, names_db_post, return_indices = True)\n",
    "\n",
    "print('There are {} post-merger galaxies with flag-free morphology data.'.format(len(names_db_post[idx_post])))\n",
    "\n",
    "names_db_post = names_db_post[idx_post]#[0:len(Tum_pre[idx_pre])]\n",
    "Mstar_post = Mstar_post[idx_post]#[0:len(Tum_pre[idx_pre])]\n",
    "Tpm_post = Tpm_post[idx_post]#[0:len(Tum_pre[idx_pre])]\n",
    "mu_post = mu_post[idx_post]#[0:len(Tum_pre[idx_pre])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450bb45",
   "metadata": {},
   "source": [
    "# Preparing Data for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299b0152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all merging galaxies\n",
    "names_mergers = np.array(list(names_db_pre) + list(names_db_post))\n",
    "T_merger = np.array(list(Tum_pre) + list(Tpm_post))\n",
    "mu = np.array(list(mu_pre) + list(mu_post))\n",
    "mass = np.array(list(Mstar_pre) + list(Mstar_post))\n",
    "\n",
    "# match catalogues\n",
    "match, idx_morph, idx_merge = np.intersect1d(names_morph, names_mergers, return_indices = True)\n",
    "\n",
    "T_merger = T_merger[idx_merge]\n",
    "morph = morph[:,idx_morph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1db2154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 5975)\n"
     ]
    }
   ],
   "source": [
    "# shape of output\n",
    "print(morph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aaa2f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5975,)\n"
     ]
    }
   ],
   "source": [
    "# shape of output\n",
    "print(T_merger.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60be4edd",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f1943b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and validation sets (75/25 split)\n",
    "input_tr,input_va,target_tr, target_va = train_test_split(morph.T, T_merger,test_size=0.25, shuffle = True)\n",
    "\n",
    "\n",
    "target_tr_orig = target_tr.copy()\n",
    "target_va_orig = target_va.copy()\n",
    "\n",
    "# prep for classification, ie. change T_merger to binary; 0 if pre-merger, 1 if post-merger\n",
    "target_tr[target_tr<0] = 0\n",
    "target_tr[target_tr>0] = 1\n",
    "target_va[target_va<0] = 0\n",
    "target_va[target_va>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20fa4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()),\\\n",
    "                 ('pca', PCA()),\\\n",
    "                 ('LR', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1c58f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('LR', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(input_tr, target_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f91815",
   "metadata": {},
   "source": [
    "## We can use the pipeline we've made with gridsearch to easily find the best hyerparameters for the model and best data preparation choices (e.g. n_components for PCA) to solve our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33428aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid of parameters to try\n",
    "\n",
    "# type of penalty term applied\n",
    "penalty  = ['l1', 'l2']\n",
    "\n",
    "# Algorithm to use in the optimization problem\n",
    "solver = ['liblinear', 'saga']\n",
    "\n",
    "# Tolerance for stopping criteria\n",
    "tol = [1., 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "# Inverse of regularization strength, smaller values specify stronger regularization.\n",
    "C = [0.01, 0.1, 0.25, 0.5, 1.0, 1.5 , 2, 10]\n",
    "\n",
    "# Maximum number of iterations taken for the solvers to converge.\n",
    "max_iter = [1, 10, 50, 100, 150, 200, 500]\n",
    "\n",
    "#n_components = [1,5,10,20,30,50,75,100,125,150]\n",
    "n_components = [1,2,3,4,5,6,7]\n",
    "\n",
    "#param_grid = [{\"penalty\": penalty, \"solver\": solver, \"tol\": tol, \"C\": C, \"max_iter\": max_iter }]\n",
    "param_grid = [{\"pca__n_components\": n_components, \"LR__penalty\": penalty, \"LR__solver\": solver, \"LR__tol\": tol, \"LR__C\": C, \"LR__max_iter\": max_iter}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f59dbd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LR__C': 0.5, 'LR__max_iter': 1, 'LR__penalty': 'l1', 'LR__solver': 'saga', 'LR__tol': 0.0001, 'pca__n_components': 5}\n",
      "0.7190359294800268\n"
     ]
    }
   ],
   "source": [
    "gs = GridSearchCV(pipe, param_grid, cv = 5)\n",
    "\n",
    "gs.fit(input_tr, target_tr)\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc10267",
   "metadata": {},
   "source": [
    "Most interesting resultis:\n",
    "\n",
    "1. max_iter = 1? This would mean that multiple iterations only works to worsen the model\n",
    "2. only uses 5 PCA components, does this mean the rest of the data wasn't just not useful, but actually harmful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489340a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
